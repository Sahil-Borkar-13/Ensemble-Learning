{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968d466d",
   "metadata": {},
   "source": [
    "# **Ensemble Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea28c3",
   "metadata": {},
   "source": [
    "## **Assignment Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ed98f",
   "metadata": {},
   "source": [
    "### **Theoretical**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc83dc3",
   "metadata": {},
   "source": [
    "1. Can we use Bagging for regression problems?\n",
    "    - Yes, Bagging can be used for regression problems. In regression, the base models are typically decision trees, and the final prediction is made by averaging the predictions of all the individual trees in the ensemble. This approach is known as a Bagging Regressor.\n",
    "    - Bagging helps to reduce variance and improve the stability and accuracy of regression models by combining the predictions of multiple models trained on different subsets of the data.\n",
    "\n",
    "2. What is the difference between multiple model training and single model training?\n",
    "    - Multiple model training involves training several models on different subsets of the data or using different algorithms, and then combining their predictions to make a final decision. This is the basis of ensemble learning techniques like Bagging and Boosting.\n",
    "    - Single model training, on the other hand, involves training a single model on the entire dataset. The performance of a single model can be limited by its bias and variance, while multiple model training can help to reduce these issues by leveraging the strengths of different models.\n",
    "\n",
    "3. Explain the concept of feature randomness in Random Forest.\n",
    "    - Feature randomness in Random Forest refers to the process of selecting a random subset of features for each split in the decision trees that make up the forest. This is done to ensure that the trees are diverse and not too similar to each other, which helps to reduce overfitting and improve the generalization of the model.\n",
    "    - By introducing randomness in the feature selection process, Random Forest can create a more robust model that is less sensitive to noise in the data and can capture a wider range of patterns.\n",
    "\n",
    "4. What is OOB (Out-of-Bag) score?\n",
    "    - The OOB (Out-of-Bag) score is a method for evaluating the performance of a Random Forest model without the need for a separate validation set. It is calculated using the samples that were not included in the bootstrap sample for each tree (the \"out-of-bag\" samples).\n",
    "    - The OOB score is an estimate of the model's accuracy and can be used to assess the performance of the Random Forest without the need for cross-validation or a separate test set.\n",
    "\n",
    "5. How can you measure the importance of features in a Random Forest model?\n",
    "    - The importance of features in a Random Forest model can be measured using various methods, such as:\n",
    "        - Mean Decrease in Impurity (MDI): This method measures the total decrease in node impurity (e.g., Gini impurity or mean squared error) that results from splits on a particular feature across all trees in the forest.\n",
    "        - Mean Decrease in Accuracy (MDA): This method measures the decrease in model accuracy when the values of a particular feature are permuted. A larger decrease indicates that the feature is more important for the model's predictions.\n",
    "        - Feature importance can also be visualized using bar plots or other graphical representations to show the relative importance of each feature in the model.\n",
    "\n",
    "6. Explain the working principle of a Bagging Classifier.\n",
    "    - A Bagging Classifier works by creating multiple versions of a base classifier (e.g., decision trees) and training each version on a different random subset of the training data (with replacement, known as bootstrap sampling). Each classifier is trained independently, and the final prediction is made by aggregating the predictions of all the classifiers, typically through majority voting for classification tasks.\n",
    "    - The main idea behind Bagging is to reduce the variance of the model by averaging the predictions of multiple models, which can lead to improved performance and reduced overfitting compared to a single model.\n",
    "\n",
    "7. How do you evaluate a Bagging Classifierâ€™s performance?\n",
    "    - The performance of a Bagging Classifier can be evaluated using various metrics, such as:\n",
    "        - Accuracy: The proportion of correctly classified instances out of the total instances.\n",
    "        - Precision: The proportion of true positive predictions out of all positive predictions.\n",
    "        - Recall: The proportion of true positive predictions out of all actual positives.\n",
    "        - F1 Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "        - Confusion Matrix: A table that shows the counts of true positives, true negatives, false positives, and false negatives, which can help to understand the performance of the classifier in more detail.\n",
    "    - Additionally, cross-validation can be used to assess the model's performance on different subsets of the data and to ensure that the results are robust and not due to overfitting.\n",
    "\n",
    "8. How does a Bagging Regressor work?\n",
    "    - A Bagging Regressor works similarly to a Bagging Classifier, but it is designed for regression tasks. It creates multiple versions of a base regressor (e.g., decision trees) and trains each version on a different random subset of the training data (with replacement). Each regressor is trained independently, and the final prediction is made by averaging the predictions of all the regressors.\n",
    "    - The main idea behind a Bagging Regressor is to reduce the variance of the model by averaging the predictions of multiple models, which can lead to improved performance and reduced overfitting compared to a single model.\n",
    "\n",
    "9. What is the main advantage of ensemble techniques?\n",
    "    - The main advantage of ensemble techniques is that they can significantly improve the performance and robustness of machine learning models by combining the predictions of multiple models. Ensemble methods can reduce overfitting, increase accuracy, and provide better generalization to unseen data compared to individual models. By leveraging the strengths of different models and mitigating their weaknesses, ensemble techniques can achieve superior results in various machine learning tasks.\n",
    "\n",
    "10. What is the main challenge of ensemble methods?\n",
    "    - The main challenge of ensemble methods is that they can be computationally expensive and time-consuming to train, especially when using a large number of base models. Additionally, ensemble methods can be more complex to interpret and understand compared to single models, making it difficult to explain the predictions to stakeholders. Furthermore, if not properly tuned, ensemble methods can still suffer from overfitting, particularly if the base models are too complex or if there is not enough diversity among the models in the ensemble.\n",
    "\n",
    "11. Explain the key idea behind ensemble techniques.\n",
    "    - The key idea behind ensemble techniques is to combine the predictions of multiple models to create a more accurate and robust model. By leveraging the strengths of different models and mitigating their weaknesses, ensemble methods can achieve better performance than any individual model. The diversity among the models in the ensemble is crucial for improving performance, as it allows the ensemble to capture a wider range of patterns in the data and reduce overfitting. Ensemble techniques can be applied to both classification and regression tasks, and they are widely used in various machine learning applications to enhance predictive accuracy.\n",
    "\n",
    "12. What is a Random Forest Classifier?\n",
    "    - A Random Forest Classifier is an ensemble learning method that consists of a collection of decision trees. Each tree is trained on a random subset of the training data (with replacement) and a random subset of features for each split. The final prediction of the Random Forest Classifier is made by aggregating the predictions of all the individual trees, typically through majority voting for classification tasks. Random Forests are known for their high accuracy, robustness to overfitting, and ability to handle large datasets with many features.\n",
    "\n",
    "13. What are the main types of ensemble techniques?\n",
    "    - The main types of ensemble techniques include:\n",
    "        - Bagging (Bootstrap Aggregating): This technique involves training multiple models on different random subsets of the training data and aggregating their predictions to reduce variance and improve performance.\n",
    "        - Boosting: This technique involves sequentially training models, where each model focuses on correcting the errors of the previous model. The final prediction is made by combining the predictions of all models, often with weighted voting or averaging.\n",
    "        - Stacking: This technique involves training multiple base models and then using their predictions as input to a meta-model, which makes the final prediction. Stacking can leverage the strengths of different types of models to improve performance.\n",
    "        \n",
    "14. What is ensemble learning in machine learning?\n",
    "    - Ensemble learning in machine learning refers to the technique of combining the predictions of multiple models to create a more accurate and robust model. The idea is that by leveraging the strengths of different models and mitigating their weaknesses, ensemble methods can achieve better performance than any individual model. Ensemble learning can be applied to both classification and regression tasks, and it includes techniques such as Bagging, Boosting, and Stacking.\n",
    "\n",
    "15. When should we avoid using ensemble methods?\n",
    "    - We should avoid using ensemble methods when:\n",
    "        - The dataset is small: Ensemble methods can be prone to overfitting when the dataset is small, as they may capture noise in the data rather than the underlying patterns.\n",
    "        - Interpretability is a concern: Ensemble methods can be complex and difficult to interpret, making it challenging to explain the predictions to stakeholders.\n",
    "        - Computational resources are limited: Ensemble methods can be computationally expensive and time-consuming to train, especially when using a large number of base models.\n",
    "        - A single model performs well: If a single model already achieves high performance on the task, using an ensemble may not provide significant benefits and could add unnecessary complexity.\n",
    "\n",
    "16. How does Bagging help in reducing overfitting?\n",
    "    - Bagging helps in reducing overfitting by training multiple models on different random subsets of the training data (with replacement). This process creates diversity among the models, as each model is exposed to different samples of the data. By aggregating the predictions of these diverse models, Bagging can reduce the variance of the overall model and improve its generalization to unseen data. The averaging of predictions helps to smooth out the noise and prevent any single model from dominating the predictions, which can lead to overfitting.\n",
    "\n",
    "17. Why is Random Forest better than a single Decision Tree?\n",
    "    - Random Forest is better than a single Decision Tree because it reduces the risk of overfitting and improves the generalization of the model. A single Decision Tree can easily overfit the training data, especially if it is deep and complex. In contrast, Random Forest creates multiple decision trees using different random subsets of the data and features, which introduces diversity among the trees. By aggregating the predictions of multiple trees, Random Forest can achieve higher accuracy and robustness compared to a single Decision Tree. Additionally, Random Forest can handle large datasets with many features and is less sensitive to noise in the data.\n",
    "\n",
    "18. What is the role of bootstrap sampling in Bagging?\n",
    "    - Bootstrap sampling in Bagging is the process of creating multiple random subsets of the training data by sampling with replacement. Each subset is used to train a different base model (e.g., decision tree) in the ensemble. The role of bootstrap sampling is to introduce diversity among the models, as each model is trained on a different subset of the data. This diversity helps to reduce overfitting and improve the generalization of the ensemble model. By aggregating the predictions of these diverse models, Bagging can achieve better performance than any individual model trained on the entire dataset.\n",
    "\n",
    "19. What are some real-world applications of ensemble techniques?\n",
    "    - Some real-world applications of ensemble techniques include:\n",
    "        - Fraud detection: Ensemble methods can be used to improve the accuracy of fraud detection systems by combining the predictions of multiple models.\n",
    "        - Customer churn prediction: Ensemble techniques can help to predict customer churn by leveraging the strengths of different models to capture complex patterns in customer behavior.\n",
    "        - Image classification: Ensemble methods can be used to improve the performance of image classification tasks by combining the predictions of multiple models trained on different features or architectures.\n",
    "        - Natural language processing: Ensemble techniques can enhance the performance of NLP tasks such as sentiment analysis, text classification, and machine translation by combining the predictions of multiple models.\n",
    "        - Medical diagnosis: Ensemble methods can assist in medical diagnosis by combining the predictions of multiple models trained on different types of medical data, such as imaging, lab results, and patient history.\n",
    "\n",
    "20. What is the difference between Bagging and Boosting?\n",
    "    - Bagging and Boosting are both ensemble techniques, but they differ in their approach:\n",
    "        - Bagging (Bootstrap Aggregating) trains multiple models independently on different random subsets of the data and combines their predictions by averaging (for regression) or majority voting (for classification). It aims to reduce variance and overfitting.\n",
    "        - Boosting trains models sequentially, where each model tries to correct the errors of the previous models. It focuses on reducing bias by giving more weight to misclassified samples in subsequent iterations. Examples include AdaBoost, Gradient Boosting, and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c671d",
   "metadata": {},
   "source": [
    "### **Practical Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07fee05",
   "metadata": {},
   "source": [
    "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1366936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d74df",
   "metadata": {},
   "source": [
    "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facdde03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3256.961797752809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f09406",
   "metadata": {},
   "source": [
    "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc12525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius : 0.034843233980943286\n",
      "mean texture : 0.015225145712914773\n",
      "mean perimeter : 0.06799034063826767\n",
      "mean area : 0.0604616365111208\n",
      "mean smoothness : 0.0079584528113981\n",
      "mean compactness : 0.0115970382551153\n",
      "mean concavity : 0.06691736463414073\n",
      "mean concave points : 0.10704565721708294\n",
      "mean symmetry : 0.0034227883667066654\n",
      "mean fractal dimension : 0.002615076161734035\n",
      "radius error : 0.014263704023561991\n",
      "texture error : 0.003744265527721131\n",
      "perimeter error : 0.010085060356218195\n",
      "area error : 0.029552828963121246\n",
      "smoothness error : 0.0047215698751171715\n",
      "compactness error : 0.0056118342607874684\n",
      "concavity error : 0.005819693803295157\n",
      "concave points error : 0.0037597476696419278\n",
      "symmetry error : 0.003545970882211007\n",
      "fractal dimension error : 0.005942332118800315\n",
      "worst radius : 0.08284828183729644\n",
      "worst texture : 0.017485260960677165\n",
      "worst perimeter : 0.08084969717184524\n",
      "worst area : 0.13935694286788813\n",
      "worst smoothness : 0.012232023199117627\n",
      "worst compactness : 0.01986385650955019\n",
      "worst concavity : 0.03733871230020869\n",
      "worst concave points : 0.13222508566399135\n",
      "worst symmetry : 0.008179084261284875\n",
      "worst fractal dimension : 0.004497313458240299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "features = data.feature_names\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "importance = model.feature_importances_\n",
    "\n",
    "for i in range(len(features)):\n",
    "    print(features[i], \":\", importance[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf9885",
   "metadata": {},
   "source": [
    "24.  Train a Random Forest Regressor and compare its performance with a single Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db915f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MSE: 4976.797752808989\n",
      "Random Forest MSE: 2952.0105887640448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Decision Tree MSE:\", mse_dt)\n",
    "print(\"Random Forest MSE:\", mse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a07c25",
   "metadata": {},
   "source": [
    "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4649916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.9666080843585237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"OOB Score:\", model.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fabb4",
   "metadata": {},
   "source": [
    "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2792bd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7eed1",
   "metadata": {},
   "source": [
    "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5135ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 10 Accuracy: 1.0\n",
      "Trees: 50 Accuracy: 1.0\n",
      "Trees: 100 Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "trees = [10, 50, 100]\n",
    "\n",
    "for n in trees:\n",
    "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Trees:\", n, \"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58553632",
   "metadata": {},
   "source": [
    "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d77f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9980347199475925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = BaggingClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=10, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"AUC Score:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279e3f9",
   "metadata": {},
   "source": [
    "29. Train a Random Forest Regressor and analyze feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d9974f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age : 0.05749630990691616\n",
      "sex : 0.011900939045372818\n",
      "bmi : 0.2762487694420927\n",
      "bp : 0.08708509584044875\n",
      "s1 : 0.04725056648926529\n",
      "s2 : 0.05536752264925017\n",
      "s3 : 0.05119054384148261\n",
      "s4 : 0.027055593119826065\n",
      "s5 : 0.31562931724463833\n",
      "s6 : 0.07077534242070717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "features = data.feature_names\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "importance = model.feature_importances_\n",
    "\n",
    "for i in range(len(features)):\n",
    "    print(features[i], \":\", importance[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8224075",
   "metadata": {},
   "source": [
    "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b58bf5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 1.0\n",
      "Random Forest Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bag.fit(X_train, y_train)\n",
    "y_pred_bag = bag.predict(X_test)\n",
    "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Bagging Accuracy:\", acc_bag)\n",
    "print(\"Random Forest Accuracy:\", acc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dc79c",
   "metadata": {},
   "source": [
    "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d9ba3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 200}\n",
      "Best Score: 0.9714285714285715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "params = {\n",
    "    'n_estimators':[50,100,200],\n",
    "    'max_depth':[None,5,10],\n",
    "    'criterion':['gini','entropy']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, params, cv=5)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b0931",
   "metadata": {},
   "source": [
    "32. Train a Bagging Regressor with different numbers of base estimators and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d85bc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators: 5 MSE: 3367.1020224719105\n",
      "Estimators: 10 MSE: 3256.961797752809\n",
      "Estimators: 50 MSE: 3056.494602247191\n",
      "Estimators: 100 MSE: 2970.863235955056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "estimators = [5, 10, 50,100]\n",
    "\n",
    "for n in estimators:\n",
    "    model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Estimators:\", n, \"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9f752",
   "metadata": {},
   "source": [
    "33.  Train a Random Forest Classifier and analyze misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d068f58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Misclassified Samples: 4\n",
      "Misclassified Samples:\n",
      "[[1.334e+01 1.586e+01 8.649e+01 5.200e+02 1.078e-01 1.535e-01 1.169e-01\n",
      "  6.987e-02 1.942e-01 6.902e-02 2.860e-01 1.016e+00 1.535e+00 1.296e+01\n",
      "  6.794e-03 3.575e-02 3.980e-02 1.383e-02 2.134e-02 4.603e-03 1.553e+01\n",
      "  2.319e+01 9.666e+01 6.149e+02 1.536e-01 4.791e-01 4.858e-01 1.708e-01\n",
      "  3.527e-01 1.016e-01]\n",
      " [1.380e+01 1.579e+01 9.043e+01 5.841e+02 1.007e-01 1.280e-01 7.789e-02\n",
      "  5.069e-02 1.662e-01 6.566e-02 2.787e-01 6.205e-01 1.957e+00 2.335e+01\n",
      "  4.717e-03 2.065e-02 1.759e-02 9.206e-03 1.220e-02 3.130e-03 1.657e+01\n",
      "  2.086e+01 1.103e+02 8.124e+02 1.411e-01 3.542e-01 2.779e-01 1.383e-01\n",
      "  2.589e-01 1.030e-01]\n",
      " [1.396e+01 1.705e+01 9.143e+01 6.024e+02 1.096e-01 1.279e-01 9.789e-02\n",
      "  5.246e-02 1.908e-01 6.130e-02 4.250e-01 8.098e-01 2.563e+00 3.574e+01\n",
      "  6.351e-03 2.679e-02 3.119e-02 1.342e-02 2.062e-02 2.695e-03 1.639e+01\n",
      "  2.207e+01 1.081e+02 8.260e+02 1.512e-01 3.262e-01 3.209e-01 1.374e-01\n",
      "  3.068e-01 7.957e-02]\n",
      " [1.448e+01 2.146e+01 9.425e+01 6.482e+02 9.444e-02 9.947e-02 1.204e-01\n",
      "  4.938e-02 2.075e-01 5.636e-02 4.204e-01 2.220e+00 3.301e+00 3.887e+01\n",
      "  9.369e-03 2.983e-02 5.371e-02 1.761e-02 2.418e-02 3.249e-03 1.621e+01\n",
      "  2.925e+01 1.084e+02 8.089e+02 1.306e-01 1.976e-01 3.349e-01 1.225e-01\n",
      "  3.020e-01 6.846e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "misclassified = X_test[y_test != y_pred]\n",
    "\n",
    "print(\"Number of Misclassified Samples:\", len(misclassified))\n",
    "print(\"Misclassified Samples:\")\n",
    "print(misclassified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69950a23",
   "metadata": {},
   "source": [
    "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caa9a925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bag.fit(X_train, y_train)\n",
    "y_pred_bag = bag.predict(X_test)\n",
    "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", acc_dt)\n",
    "print(\"Bagging Accuracy:\", acc_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7baf10",
   "metadata": {},
   "source": [
    "35. Train a Random Forest Classifier and visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62f77132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab100c",
   "metadata": {},
   "source": [
    "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61d412d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "SVM Accuracy: 1.0\n",
      "Logistic Regression Accuracy: 1.0\n",
      "Stacking Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "svm = SVC(probability=True)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "estimators = [('dt', DecisionTreeClassifier()), ('svm', SVC(probability=True))]\n",
    "\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
    "\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "y_pred_stack = stack.predict(X_test)\n",
    "\n",
    "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", acc_dt)\n",
    "print(\"SVM Accuracy:\", acc_svm)\n",
    "print(\"Logistic Regression Accuracy:\", acc_lr)\n",
    "print(\"Stacking Accuracy:\", acc_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5bd901",
   "metadata": {},
   "source": [
    "37. Train a Random Forest Classifier and print the top 5 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fe34128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst area : 0.13935694286788813\n",
      "worst concave points : 0.13222508566399135\n",
      "mean concave points : 0.10704565721708294\n",
      "worst radius : 0.08284828183729644\n",
      "worst perimeter : 0.08084969717184524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "features = data.feature_names\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "importance = model.feature_importances_\n",
    "\n",
    "indices = importance.argsort()[::-1]\n",
    "\n",
    "for i in range(5):\n",
    "    print(features[indices[i]], \":\", importance[indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7ee6c",
   "metadata": {},
   "source": [
    "38.  Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc4af912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9583333333333334\n",
      "Recall: 0.971830985915493\n",
      "F1-score: 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc823d",
   "metadata": {},
   "source": [
    "39.  Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5ac023c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2 Accuracy: 1.0\n",
      "Max Depth: 5 Accuracy: 1.0\n",
      "Max Depth: 10 Accuracy: 1.0\n",
      "Max Depth: None Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "depth = [2, 5, 10, None]\n",
    "\n",
    "for d in depth:\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=d, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Max Depth:\", d, \"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41efb17",
   "metadata": {},
   "source": [
    "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f93ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MSE: 3256.961797752809\n",
      "KNeighbors MSE: 2990.653649438202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model1 = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_test)\n",
    "mse1 = mean_squared_error(y_test, y_pred1)\n",
    "\n",
    "model2 = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=10, random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "mse2 = mean_squared_error(y_test, y_pred2)\n",
    "\n",
    "print(\"Decision Tree MSE:\", mse1)\n",
    "print(\"KNeighbors MSE:\", mse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c5dd0",
   "metadata": {},
   "source": [
    "41.  Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64b18824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.9952505732066819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"ROC-AUC Score:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07bc58",
   "metadata": {},
   "source": [
    "42.  Train a Bagging Classifier and evaluate its performance using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5114fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
      "Mean Accuracy: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903c138",
   "metadata": {},
   "source": [
    "43. Train a Random Forest Classifier and plot the Precision-Recall curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b55da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9368f",
   "metadata": {},
   "source": [
    "44.  Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6559ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n",
      "Logistic Regression Accuracy: 1.0\n",
      "Stacking Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "estimators = [('rf', RandomForestClassifier(n_estimators=100, random_state=42))]\n",
    "\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
    "\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "y_pred_stack = stack.predict(X_test)\n",
    "\n",
    "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", acc_rf)\n",
    "print(\"Logistic Regression Accuracy:\", acc_lr)\n",
    "print(\"Stacking Accuracy:\", acc_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e96781",
   "metadata": {},
   "source": [
    "45.  Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af147d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Samples: 0.5 MSE: 3235.3548314606737\n",
      "Max Samples: 0.7 MSE: 3248.8026966292127\n",
      "Max Samples: 1.0 MSE: 3256.961797752809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "samples = [0.5, 0.7, 1.0]\n",
    "\n",
    "for s in samples:\n",
    "    model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, max_samples=s, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Max Samples:\", s, \"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6fe8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
